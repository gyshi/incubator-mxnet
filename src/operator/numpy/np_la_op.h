/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

/*!
 * Copyright (c) 2019 by Contributors
 * \file np_la_op.h
 * \brief Function definition of Operators for advanced linear algebra.
 */
#ifndef MXNET_OPERATOR_NUMPY_NP_LA_OP_H_
#define MXNET_OPERATOR_NUMPY_NP_LA_OP_H_

#include <tuple>
#include "../tensor/broadcast_reduce_op.h"
#include "../tensor/broadcast_reduce-inl.h"


namespace mxnet {
namespace op {


using namespace mshadow;
using namespace broadcast;

struct NumpyLaNormParam : public dmlc::Parameter<NumpyLaNormParam> {
  double ord;
  dmlc::optional<mxnet::TShape> axis;
  bool keepdims;
  int flag;
  DMLC_DECLARE_PARAMETER(NumpyLaNormParam) {
      DMLC_DECLARE_FIELD(ord).set_default(2)
          .describe("Order of the norm. inf means numpyâ€™s inf object.");
      DMLC_DECLARE_FIELD(axis).set_default(dmlc::optional<mxnet::TShape>())
      .describe(R"code(If axis is an integer, it specifies the axis of x along
         which to compute the vector norms. If axis is a 2-tuple,
         it specifies the axes that hold 2-D matrices, and the matrix norms of
         these matrices are computed. If axis is None then either a vector norm (when x is 1-D)
         or a matrix norm (when x is 2-D) is returned.If axis is an integer,
         it specifies the axis of x along which to compute the vector norms.
         If axis is a 2-tuple, it specifies the axes that hold 2-D matrices,
         and the matrix norms of these matrices are computed. If axis is None then either a
         vector norm (when x is 1-D) or a matrix norm (when x is 2-D) is returned.)code");
      DMLC_DECLARE_FIELD(keepdims).set_default(false)
      .describe("If this is set to `True`, the reduced axis is left "
      "in the result as dimension with size one.");
      DMLC_DECLARE_FIELD(flag).set_default(-1)
      .describe("{ord: None  flag:0, ord: 'fro'  flag:1, ord: 'nuc'  flag:2"
      " ord: inf  flag:3  ord: -inf  flag:4}");
  }
};

inline mxnet::TShape NumpyNormReduceAxesShapeImpl(const mxnet::TShape& ishape,
                                                  const dmlc::optional<mxnet::TShape>& axis,
                                                      bool keepdims, bool exclude) {
  // if axis doesn't have value, treat it same mxnet::TShape(0).
  if (!axis.has_value() || axis.value().ndim() == 0) {
    if (keepdims) {
      return mxnet::TShape(ishape.ndim(), 1);
    } else {
      return mxnet::TShape(0, -1);
    }
  }
  // axis has value
  mxnet::TShape axes(axis.value());
  for (index_t i = 0; i < axes.ndim(); i++) {
    if (axes[i] < 0) {
      axes[i] += ishape.ndim();
    }
  }
  std::sort(axes.begin(), axes.end());

  for (index_t i = 1; i < axes.ndim(); i++) {
    CHECK_LT(axes[i-1], axes[i])
      << "Reduction axes have duplicates "
      << axes;
  }
  CHECK_LT(axes[axes.ndim()-1], ishape.ndim())
    << "Reduction axis " << axes[axes.ndim()-1]
    << " Exceeds input dimensions " << ishape;
  CHECK_GE(axes[0], 0)
    << "Reduction axis " << axis.value()
    << " Exceeds input dimensions " << ishape;

  mxnet::TShape oshape;
  if (keepdims) {
    oshape = mxnet::TShape(ishape);
  } else if (exclude) {
    oshape = mxnet::TShape(axes.ndim(), 1);
  } else {
    oshape = mxnet::TShape(std::max(1, ishape.ndim() - axes.ndim()), 1);
  }

  if (keepdims && exclude) {
    for (index_t i = 0, j = 0; i < ishape.ndim(); ++i) {
      if (j < axes.ndim() && i == axes[j]) {
        ++j;
        continue;
      }
      oshape[i] = 1;
    }
  } else if (keepdims) {
    for (index_t i = 0; i < axes.ndim(); ++i) {
      oshape[axes[i]] = 1;
    }
  } else if (exclude) {
    for (index_t i = 0; i < axes.ndim(); ++i) {
      oshape[i] = ishape[axes[i]];
    }
  } else {
    for (index_t i = 0, j = 0, k = 0; i < ishape.ndim(); ++i) {
      if (j < axes.ndim() && i == axes[j]) {
        ++j;
        continue;
      }
      oshape[k++] = ishape[i];
    }
  }
  return oshape;
}


inline bool NumpyLaNormShape(const nnvm::NodeAttrs& attrs,
                             mxnet::ShapeVector *in_attrs,
                             mxnet::ShapeVector *out_attrs) {
  CHECK_EQ(in_attrs->size(), 1U);
  CHECK_EQ(out_attrs->size(), 1U);
  if (!shape_is_known((*in_attrs)[0])) return false;
  const NumpyLaNormParam& param = nnvm::get<NumpyLaNormParam>(attrs.parsed);
  const int ndim = (*in_attrs)[0].ndim();
  if ((!param.axis.has_value() && param.flag != 0 && ndim > 2)
       || (param.axis.has_value() && param.axis.value().ndim() > 2))
    LOG(FATAL) << "Improper number of dimensions to norm.";

  if (!param.axis.has_value()) {
    if ((ndim == 0 && param.flag != 0) ||    // for scalar
        (ndim == 1 && (param.flag == 1 || param.flag ==2)) ||
        (ndim >= 2 && (param.ord == 0 || param.ord > 2 || param.ord < -2))) {
      LOG(FATAL) << "Invalid norm order for inputs.";
    }
  } else {
    if ((param.axis.value().ndim() == 0 && param.flag != 0) ||    // for scalar
        (param.axis.value().ndim() == 1 && (param.flag == 1 || param.flag ==2)) ||
        (param.axis.value().ndim() == 2 && (param.ord == 0 || param.ord > 2 || param.ord < -2))) {
      LOG(FATAL) << "Invalid norm order for inputs.";
    }
  }

  SHAPE_ASSIGN_CHECK(*out_attrs, 0,
                     NumpyNormReduceAxesShapeImpl((*in_attrs)[0], param.axis, param.keepdims, false));
  return true;
}

/*! \brief compute ln norm */
struct nrmn {
  /*! \brief do reduction into dst */
  template<typename AType, typename DType>
  MSHADOW_XINLINE static void Reduce(volatile AType& sum_of_power, volatile DType src, double scale) { // NOLINT(*)
    AType c;
    c = math::pow(src, scale);
    sum_of_power += c;
  }
  /*! \brief finalize reduction result */
  template<typename DType>
  MSHADOW_XINLINE static void Finalize(volatile DType& sum_of_power, double scale) { // NOLINT(*)
    sum_of_power = math::pow(sum_of_power, 1/scale);
  }
  /*! \brief combine the results of two reducers */
  template<typename DType>
  MSHADOW_XINLINE static void Merge(volatile DType& dst_val, volatile DType& dst_residual, volatile DType& src_val, volatile DType& scale) { // NOLINT(*)
    dst_val += src_val;
  }
  /*!
   *\brief set the initial value during reduction
   */
  template<typename DType>
  MSHADOW_XINLINE static void SetInitValue(DType &sum_of_power) { // NOLINT(*)
    sum_of_power = 0;
  }
  /*!
   *\brief set the initial value during reduction
   */
  template<typename DType>
  MSHADOW_XINLINE static void SetInitValue(DType &sum_of_power, double scale) { // NOLINT(*)
    SetInitValue(sum_of_power);
  }
};

struct nrm_zero {
  /*! \brief do reduction into dst */
  template<typename DType>
  MSHADOW_XINLINE static void Reduce(volatile DType& dst,  volatile DType src) { // NOLINT(*)
    DType res = src == 0 ? 0 : 1;
    dst +=  res;
  }
  /*! \brief do reduction into dst */
  template<typename DType>
  MSHADOW_XINLINE static void Reduce(volatile DType& dst,  volatile DType src, volatile DType &none) { // NOLINT(*)
    Reduce(dst, src);
  }
  template<typename DType>
  MSHADOW_XINLINE static void Merge(volatile DType& dst_val, volatile DType& src_val) { // NOLINT(*)
    Reduce(dst_val, src_val);
  }
  /*! \brief combine the results of two reducers */
  template<typename DType>
  MSHADOW_XINLINE static void Merge(volatile DType& dst_val, volatile DType& dst_residual, volatile DType& src_val, volatile DType& src_residual) { // NOLINT(*)
    Reduce(dst_val, src_val);
  }
  /*! \brief finalize reduction */
  template<typename DType>
  MSHADOW_XINLINE static void Finalize(volatile DType& dst) {} // NOLINT(*)
  /*! \brief finalize reduction */
  template<typename DType>
  MSHADOW_XINLINE static void Finalize(volatile DType& dst, volatile DType& residual) {} // NOLINT(*
  /*!
   *\brief set the initial value during reduction
   */
  /*!
*\brief calculate gradient of redres with respect to redsrc,
* redres: reduced result, redsrc: one of reduction element
*/
  template<typename DType>
  MSHADOW_XINLINE static DType PartialGrad(DType redres, DType redsrc) {
    return 1;
  }
  template<typename DType>
  MSHADOW_XINLINE static void SetInitValue(DType &initv) { // NOLINT(*)
    initv = 0;
  }
  /*!
   *\brief set the initial value during reduction
   */
  template<typename DType>
  MSHADOW_XINLINE static void SetInitValue(DType &initv, DType &none) { // NOLINT(*)
    SetInitValue(initv);
  }
};

template<typename Reducer, int ndim, typename AType, typename DType, typename OType, typename OP>
void norm_seq_reduce_compute(const size_t N, const size_t M, const bool addto,
                             const DType *big, OType *small, const Shape<ndim> bshape,
                             const Shape<ndim> sshape, const Shape<ndim> rshape,
                             const Shape<ndim> rstride, const double ord) {
  #pragma omp parallel for num_threads(engine::OpenMP::Get()->GetRecommendedOMPThreadCount())
  for (index_t idx = 0; idx < static_cast<index_t>(N); ++idx) {
    Shape<ndim> coord = unravel(idx, sshape);
    index_t j = ravel(coord, bshape);
    AType val;
    double residual = ord;
    Reducer::SetInitValue(val, residual);
    for (size_t k = 0; k < M; ++k) {
      coord = unravel(k, rshape);
      Reducer::Reduce(val, AType(OP::Map(big[j + dot(coord, rstride)])), residual);
    }
    Reducer::Finalize(val, residual);
    assign(&small[idx], addto, OType(val));
  }
}
#ifdef __CUDACC__
#include "np_la_op-inl.cuh"

#else

template <typename Reducer, int ndim, typename DType, typename OP, bool safe_acc = false>
void PowerReduce(Stream<cpu>* s, const TBlob& small, const OpReqType req,
            const Tensor<cpu, 1, char>& workspace, const TBlob& big, const double ord) {
  if (req == kNullOp) return;
  Shape<ndim> rshape, rstride;
  diff(small.shape_.get<ndim>(), big.shape_.get<ndim>(), &rshape, &rstride);
  size_t N = small.shape_.Size(), M = rshape.Size();
  if (!safe_acc) {
    norm_seq_reduce_compute<Reducer, ndim, DType, DType, DType, OP>(
        N, M, req == kAddTo, big.dptr<DType>(), small.dptr<DType>(),
        big.shape_.get<ndim>(), small.shape_.get<ndim>(), rshape, rstride, ord);
  } else {
    // Use real-only type swtich for windows temporarily due to CI issues.
#ifndef _WIN32
    MXNET_ACC_TYPE_SWITCH(mshadow::DataType<DType>::kFlag, DataType, AType, {
      typedef typename std::conditional<safe_acc, AType, DataType>::type AccType;
      MSHADOW_TYPE_SWITCH(small.type_flag_, OType, {
          typedef typename std::conditional<safe_acc, OType, DataType>::type OutType;
          norm_seq_reduce_compute<Reducer, ndim, AccType, DataType, OutType, OP>(
          N, M, req == kAddTo, big.dptr<DataType>(), small.dptr<OutType>(),
          big.shape_.get<ndim>(), small.shape_.get<ndim>(), rshape, rstride, ord);
      });
    });
#else
    MXNET_REAL_ACC_TYPE_SWITCH(mshadow::DataType<DType>::kFlag, DataType, AType, {
      typedef typename std::conditional<safe_acc, AType, DataType>::type AccType;
      MSHADOW_TYPE_SWITCH(small.type_flag_, OType, {
        typedef typename std::conditional<safe_acc, OType, DataType>::type OutType;
        norm_seq_reduce_compute<Reducer, ndim, AccType, DataType, OutType, OP>(
          N, M, req == kAddTo, big.dptr<DataType>(), small.dptr<OutType>(),
          big.shape_.get<ndim>(), small.shape_.get<ndim>(), rshape, rstride, ord);
      });
    });
#endif
  }
}


#endif


template<typename xpu, typename reducer, bool safe_acc = false, bool normalize = false,
    typename OP = op::mshadow_op::identity>
void NumpyNormPowerComputeImpl(const OpContext& ctx,
                               const std::vector<TBlob>& inputs,
                               const std::vector<OpReqType>& req,
                               const std::vector<TBlob>& outputs,
                               const mxnet::TShape& small,
                               const double& ord) {
  using namespace mshadow;
  using namespace mshadow::expr;

  mxnet::TShape src_shape, dst_shape;
  BroadcastReduceShapeCompact(inputs[0].shape_, small, &src_shape, &dst_shape);
  Stream<xpu> *s = ctx.get_stream<xpu>();
  MSHADOW_TYPE_SWITCH(inputs[0].type_flag_, DType, {
    MSHADOW_TYPE_SWITCH(outputs[0].type_flag_, OType, {
        const TBlob in_data = inputs[0].reshape(src_shape);
        const TBlob out_data = outputs[0].reshape(dst_shape);
        BROADCAST_NDIM_SWITCH(dst_shape.ndim(), NDim, {
          size_t workspace_size = broadcast::ReduceWorkspaceSize<NDim, DType>(
              s, out_data.shape_, req[0], in_data.shape_);
          Tensor<xpu, 1, char> workspace =
              ctx.requested[0].get_space_typed<xpu, 1, char>(Shape1(workspace_size), s);
          PowerReduce<reducer, NDim, DType, OP, safe_acc>(
              s, out_data, req[0], workspace, in_data, ord);
          if (normalize) {
            auto out = out_data.FlatTo2D<xpu, OType>(s);
            out /= scalar<OType>(src_shape.Size()/dst_shape.Size());
          }
        });
    });
  });
}

template<int req, typename OP, bool normzero>
struct numpy_norm_reduce_axes_backward_broadcast{
  template<typename DType, typename OType>
  MSHADOW_XINLINE static void Map(index_t i,
                                  DType *data,
                                  OType *out,
                                  DType *igrad,
                                  OType *ograd,
                                  mshadow::Shape<5> in_shape,
                                  mshadow::Shape<5> out_shape,
                                  const uint32_t ndim) {
    size_t in_stride = 1;
    size_t out_stride = 1;
    index_t idx = i;
    index_t out_idx = i;
    for (int iter = ndim - 1; iter >= 0; --iter) {
      size_t dim_idx = idx % in_shape[iter];
      out_idx -= dim_idx * in_stride;
      if (out_shape[iter] != 1) {
        out_idx += dim_idx * out_stride;
      }
      idx /= in_shape[iter];
      in_stride *= in_shape[iter];
      out_stride *= out_shape[iter];
    }
    int flag_data = mshadow_op::sign::Map(data[i]);
    if (normzero) {
      KERNEL_ASSIGN(igrad[i], req, DType(ograd[out_idx]) *
          DType(OP::Map(DType(flag_data*data[i]), DType(0))*flag_data));
    } else {
      KERNEL_ASSIGN(igrad[i], req, DType(ograd[out_idx]) * DType(OP::Map(DType(flag_data*data[i]), DType(out[out_idx]))*flag_data));
    }
  }
};

template <int req, typename OP>
struct numpy_norm_power_backward_broadcast {
  template<typename DType, typename OType>
  MSHADOW_XINLINE static void Map(index_t i,
                                  DType *data,
                                  OType *out,
                                  DType *igrad,
                                  OType *ograd,
                                  mshadow::Shape<5> in_shape,
                                  mshadow::Shape<5> out_shape,
                                  const uint32_t ndim,
                                  const double ord) {
    size_t in_stride = 1;
    size_t out_stride = 1;
    index_t idx = i;
    index_t out_idx = i;
    for (int iter = ndim - 1; iter >= 0; --iter) {
      size_t dim_idx = idx % in_shape[iter];
      out_idx -= dim_idx * in_stride;
      if (out_shape[iter] != 1) {
        out_idx += dim_idx * out_stride;
      }
      idx /= in_shape[iter];
      in_stride *= in_shape[iter];
      out_stride *= out_shape[iter];
    }
    int flag_data = mshadow_op::sign::Map(data[i]);
    KERNEL_ASSIGN(igrad[i], req, DType(ograd[out_idx]) * DType(out[out_idx])*
        DType(math::pow(DType(OP::Map(data[i])), ord - 2)) * DType(flag_data));
  }
};

template<typename xpu, typename OP, bool normalize = false, bool normzero = false>
void NumpyNormReduceAxesBackwardUseInOutImpl(const OpContext& ctx,
                                             const mxnet::TShape &small,
                                             const std::vector<TBlob>& inputs,
                                             const std::vector<OpReqType>& req,
                                             const std::vector<TBlob>& outputs) {
  using namespace mshadow;
  using namespace mshadow::expr;
  using namespace mxnet_op;

  mxnet::TShape src_shape, dst_shape;
  BroadcastReduceShapeCompact(outputs[0].shape_, small, &src_shape, &dst_shape);
  Stream<xpu> *s = ctx.get_stream<xpu>();

  MSHADOW_TYPE_SWITCH(inputs[0].type_flag_, DType, {
      MSHADOW_TYPE_SWITCH(outputs[0].type_flag_, OType, {
          mshadow::Shape<5> in_shape;
          mshadow::Shape<5> out_shape;
          for (int i = 0; i < 5; ++i) {
            if (i < dst_shape.ndim()) {
              in_shape[i] = src_shape[i];
              out_shape[i] = dst_shape[i];
            } else {
              in_shape[i] = 1;
              out_shape[i] = 1;
            }
          }
          if (dst_shape.ndim() == 2) {
            Tensor<xpu, 2, OType> igrad =
                outputs[0].get_with_shape<xpu, 2, OType>(src_shape.get<2>(), s);
            Tensor<xpu, 2, DType> ograd =
                inputs[0].get_with_shape<xpu, 2, DType>(dst_shape.get<2>(), s);
            Tensor<xpu, 2, OType> data =
                inputs[1].get_with_shape<xpu, 2, OType>(src_shape.get<2>(), s);
            Tensor<xpu, 2, DType> out =
                inputs[2].get_with_shape<xpu, 2, DType>(dst_shape.get<2>(), s);
            MXNET_REQ_TYPE_SWITCH(req[0], Req, {
              Kernel<numpy_norm_reduce_axes_backward_broadcast<Req, OP, normzero>, xpu>::Launch(
                  s, outputs[0].shape_.Size(), data.dptr_, out.dptr_, igrad.dptr_, ograd.dptr_,
                  in_shape, out_shape, src_shape.ndim());
            });
            if (normalize) igrad /= scalar<OType>(src_shape.Size()/dst_shape.Size());
          } else {
            const int ndim = MXNET_SPECIAL_MAX_NDIM;
            Tensor<xpu, ndim, OType> igrad =
                outputs[0].get_with_shape<xpu, ndim, OType>(src_shape.get<ndim>(), s);
            Tensor<xpu, ndim, DType> ograd =
                inputs[0].get_with_shape<xpu, ndim, DType>(dst_shape.get<ndim>(), s);
            Tensor<xpu, ndim, OType> data =
                inputs[1].get_with_shape<xpu, ndim, OType>(src_shape.get<ndim>(), s);
            Tensor<xpu, ndim, DType> out =
                inputs[2].get_with_shape<xpu, ndim, DType>(dst_shape.get<ndim>(), s);
            MXNET_REQ_TYPE_SWITCH(req[0], Req, {
              Kernel<numpy_norm_reduce_axes_backward_broadcast<Req, OP, normzero>, xpu>::Launch(
                  s, outputs[0].shape_.Size(), data.dptr_, out.dptr_, igrad.dptr_, ograd.dptr_,
                  in_shape, out_shape, src_shape.ndim());
            });
            if (normalize) igrad /= scalar<OType>(src_shape.Size()/dst_shape.Size());
          }
      });
  });
}

template<typename xpu, typename OP, bool normalize = false>
void NumpyNormPowerBackwardUseInOutImpl(const OpContext& ctx,
                                        const mxnet::TShape &small,
                                        const std::vector<TBlob>& inputs,
                                        const std::vector<OpReqType>& req,
                                        const std::vector<TBlob>& outputs,
                                        const double ord) {
  using namespace mshadow;
  using namespace mshadow::expr;
  using namespace mxnet_op;

  mxnet::TShape src_shape, dst_shape;
  BroadcastReduceShapeCompact(outputs[0].shape_, small, &src_shape, &dst_shape);
  Stream<xpu> *s = ctx.get_stream<xpu>();

  MSHADOW_TYPE_SWITCH(inputs[0].type_flag_, DType, {
    MSHADOW_TYPE_SWITCH(outputs[0].type_flag_, OType, {
        mshadow::Shape<5> in_shape;
        mshadow::Shape<5> out_shape;
        for (int i = 0; i < 5; ++i) {
          if (i < dst_shape.ndim()) {
            in_shape[i] = src_shape[i];
            out_shape[i] = dst_shape[i];
          } else {
            in_shape[i] = 1;
            out_shape[i] = 1;
          }
        }
        if (dst_shape.ndim() == 2) {
          Tensor<xpu, 2, OType> igrad =
              outputs[0].get_with_shape<xpu, 2, OType>(src_shape.get<2>(), s);
          Tensor<xpu, 2, DType> ograd =
              inputs[0].get_with_shape<xpu, 2, DType>(dst_shape.get<2>(), s);
          Tensor<xpu, 2, OType> data =
              inputs[1].get_with_shape<xpu, 2, OType>(src_shape.get<2>(), s);
          Tensor<xpu, 2, DType> out =
              inputs[2].get_with_shape<xpu, 2, DType>(dst_shape.get<2>(), s);
          MXNET_REQ_TYPE_SWITCH(req[0], Req, {
              Kernel<numpy_norm_power_backward_broadcast<Req, OP>, xpu>::Launch(
                  s, outputs[0].shape_.Size(), data.dptr_, out.dptr_, igrad.dptr_, ograd.dptr_,
                  in_shape, out_shape, src_shape.ndim(), ord);
          });
          if (normalize) igrad /= scalar<OType>(src_shape.Size()/dst_shape.Size());
        } else {
          const int ndim = MXNET_SPECIAL_MAX_NDIM;
          Tensor<xpu, ndim, OType> igrad =
              outputs[0].get_with_shape<xpu, ndim, OType>(src_shape.get<ndim>(), s);
          Tensor<xpu, ndim, DType> ograd =
              inputs[0].get_with_shape<xpu, ndim, DType>(dst_shape.get<ndim>(), s);
          Tensor<xpu, ndim, OType> data =
              inputs[1].get_with_shape<xpu, ndim, OType>(src_shape.get<ndim>(), s);
          Tensor<xpu, ndim, DType> out =
              inputs[2].get_with_shape<xpu, ndim, DType>(dst_shape.get<ndim>(), s);
          MXNET_REQ_TYPE_SWITCH(req[0], Req, {
              Kernel<numpy_norm_power_backward_broadcast<Req, OP>, xpu>::Launch(
                  s, outputs[0].shape_.Size(), data.dptr_, out.dptr_, igrad.dptr_, ograd.dptr_,
                  in_shape, out_shape, src_shape.ndim(), ord);
          });
          if (normalize) igrad /= scalar<OType>(src_shape.Size()/dst_shape.Size());
        }
    });
  });
}


template<typename xpu>
void NumpyLaNormCompute(const nnvm::NodeAttrs& attrs,
                        const OpContext& ctx,
                        const std::vector<TBlob>& inputs,
                        const std::vector<OpReqType>& req,
                        const std::vector<TBlob>& outputs) {
  using namespace mshadow;
  using namespace mxnet_op;
  using namespace mshadow::expr;

  CHECK_EQ(inputs.size(), 1U);
  CHECK_EQ(outputs.size(), 1U);

  if (req[0] == kNullOp) return;
  if (outputs[0].Size() == 0U) return;
  const NumpyLaNormParam& param = nnvm::get<NumpyLaNormParam>(attrs.parsed);
  const int ndim = inputs[0].ndim();

  mxnet::TShape small;
  mxnet::TShape mid;
  bool flag_axis;
  if (!param.axis.has_value()) {
    flag_axis = false;
  } else {
    flag_axis = true;
  }
  if (param.keepdims) {
    small = outputs[0].shape_;
  } else {
    small = NumpyNormReduceAxesShapeImpl(inputs[0].shape_, param.axis, true, false);
  }
  // Immediately handle some default, simple, fast, and common cases.
  if (!flag_axis && (param.flag == 0 || (param.flag == 1 && ndim == 2) ||
      (param.flag == -1 && param.ord == 2 && ndim == 1))) {
    ReduceAxesComputeImpl<xpu, mshadow_op::nrm2, false, false, mshadow_op::identity>(
            ctx, inputs, req, outputs, small);
    return;
  }
  if ((flag_axis && param.axis.value().ndim() == 1) || (!flag_axis && ndim == 1)) {
    // if ord is inf
    if (param.flag == 3) {
        ReduceAxesComputeImpl<xpu, mshadow::red::maximum, false, false, mshadow_op::abs>(
            ctx, inputs, req, outputs, small);
    } else if (param.flag == 4) {
        ReduceAxesComputeImpl<xpu, mshadow::red::minimum, false, false, mshadow_op::abs>(
            ctx, inputs, req, outputs, small);
    } else if (param.ord == 1) {
        ReduceAxesComputeImpl<xpu, mshadow_op::sum, false, false, mshadow_op::abs>(
            ctx, inputs, req, outputs, small);
    } else if (param.flag == 0 || (param.flag == -1 && param.ord == 2)) {
        ReduceAxesComputeImpl<xpu, mshadow_op::nrm2, false, false, mshadow_op::identity>(
            ctx, inputs, req, outputs, small);
    } else if (param.ord == 0) {
      ReduceAxesComputeImpl<xpu, nrm_zero, false, false, mshadow_op::identity>(
          ctx, inputs, req, outputs, small);
    } else {
        NumpyNormPowerComputeImpl<xpu, nrmn, false, false, mshadow_op::abs>(
            ctx, inputs, req, outputs, small, param.ord);
        return;
    }
  } else if ((flag_axis && param.axis.value().ndim() == 2) || (!flag_axis && ndim == 2)) {
    if (param.flag == 2 || (param.flag == -1 && (param.ord == 2 || param.ord == -2))) {
      LOG(FATAL) << "Do not implement SVD for norm.";
    } else if (param.flag == 0 || param.flag == 1) {
        ReduceAxesComputeImpl<xpu, mshadow_op::nrm2, false, false, mshadow_op::identity>(
            ctx, inputs, req, outputs, small);
    } else if (param.flag == 3 || param.flag == 4 || param.ord == 1 || param.ord == -1) {
      LOG(FATAL) << "You must give axis, like (0, 1).because it not be implemented in C++ backend.";
    } else {
      LOG(FATAL) << "Invalid norm order for matrices.";
    }
  } else {
    LOG(FATAL) << "Improper number of dimensions to norm.";
  }
}

template<typename xpu>
void NumpyLpNormGradCompute(const nnvm::NodeAttrs& attrs,
                            const OpContext& ctx,
                            const std::vector<TBlob>& inputs,
                            const std::vector<OpReqType>& req,
                            const std::vector<TBlob>& outputs) {
  using namespace mshadow;
  using namespace mshadow::expr;
  using namespace mxnet_op;
  if (req[0] == kNullOp) return;
  if (inputs[0].shape_.Size() == 0U) return;  // zero-size tensor
  const NumpyLaNormParam& param = nnvm::get<NumpyLaNormParam>(attrs.parsed);
  mxnet::TShape small;
  const int ndim = inputs[0].ndim();
  bool flag_axis;
  if (!param.axis.has_value()) {
    flag_axis = false;
  } else {
    flag_axis = true;
  }
  if (param.keepdims) {
    small = inputs[0].shape_;
  } else {
    small = NumpyNormReduceAxesShapeImpl(outputs[0].shape_, param.axis, true, false);
  }
  // Immediately handle some default, simple, fast, and common cases.
  if (!flag_axis && (param.flag == 0 || (param.flag == 1 && ndim == 2) ||
      (param.flag == -1 && param.ord == 2 && ndim == 1))) {
      ReduceAxesBackwardUseInOutImpl<xpu, mshadow_op::div, false>(ctx, small, inputs,
                                                                  req, outputs);
      return;
  }
  if ((flag_axis && param.axis.value().ndim() == 1) || (!flag_axis && ndim == 1)) {
    // if numpy ord is inf
    if (param.flag == 3 || param.flag == 4 || param.ord == 1) {
      NumpyNormReduceAxesBackwardUseInOutImpl<xpu, mshadow_op::eq, false, false>
            (ctx, small, inputs, req, outputs);
    } else if (param.flag == 0 || (param.flag == -1 && param.ord == 2)) {
      ReduceAxesBackwardUseInOutImpl<xpu, mshadow_op::div, false>(ctx, small, inputs,
                                                                  req, outputs);
    } else if (param.ord == 0) {
      NumpyNormReduceAxesBackwardUseInOutImpl<xpu, mshadow_op::ne, false, true>(ctx, small, inputs,
                                                                  req, outputs);
    } else {
      NumpyNormPowerBackwardUseInOutImpl<xpu, mshadow_op::abs, false>(ctx, small, inputs,
                                                                      req, outputs, param.ord);
      return;
    }
  } else if ((flag_axis && param.axis.value().ndim() == 2) || (!flag_axis && ndim == 2)) {
    if (param.flag == 2 || (param.flag == -1 && (param.ord == 2 || param.ord == -2))) {
      LOG(FATAL) << "Does not  implement svd for norm.";
    } else if (param.flag == 0 || param.flag == 1) {
      ReduceAxesBackwardUseInOutImpl<xpu, mshadow_op::div, false>(ctx, small, inputs,
                                                                  req, outputs);
    } else if (param.flag == 3 || param.flag == 4 ||param.ord == 1 || param.ord == -1) {
      LOG(FATAL) << "You must give axis, like (0, 1).because it not be implemented in C++ backend.";
     } else {
      LOG(FATAL) << "Invalid norm order for matrices.";
    }
  } else {
    LOG(FATAL) << "Improper number of dimensions to norm.";
  }
}

}  // namespace op
}  // namespace mxnet

#endif  // MXNET_OPERATOR_NUMPY_NP_LA_OP_H_
